class ConvGRU(nn.Module):
    def __init__(self, x_channels=3, channels=64):#X为输入图像的通到数，channels为上一隐状态的通道数
        super(ConvGRU, self).__init__()
        self.channels = channels
        self.x_channels = x_channels

        self.conv_x_z = nn.Conv2d(in_channels=self.x_channels, out_channels=self.channels, kernel_size=3, stride=1,
                                  padding=1)
        self.conv_h_z = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=3, stride=1,
                                  padding=1)
        self.conv_x_r = nn.Conv2d(in_channels=self.x_channels, out_channels=self.channels, kernel_size=3, stride=1,
                                  padding=1)

        self.conv_h_r = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=3, stride=1,
                                  padding=1)
        self.conv = nn.Conv2d(in_channels=self.x_channels, out_channels=self.channels, kernel_size=3, stride=1,
                              padding=1)

        self.conv_u = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=3, stride=1,
                                padding=1)
        # self.conv_out = nn.Conv2d(in_channels=self.channels, out_channels=self.channels, kernel_size=3, stride=1, padding=1)
        self.lReLU = nn.LeakyReLU(0.2)

    def forward(self, x, h_t_1):
        """GRU卷积流程
        args:
            x: input
            h_t_1: 上一层的隐含层输出值
        shape：
            x: [in_channels, channels, width, lenth]
        """
        z_t = F.sigmoid(self.conv_x_z(x) + self.conv_h_z(h_t_1))
        r_t = F.sigmoid((self.conv_x_r(x) + self.conv_h_r(h_t_1)))
        h_hat_t = self.lReLU(self.conv(x) + self.conv_u(torch.mul(r_t, h_t_1)))

        h_t = torch.mul((1 - z_t), h_t_1) + torch.mul(z_t, h_hat_t)
        # y = self.conv_out(h_t)
        return h_t

class resinception(nn.Module):
    def __init__(self, in_ch=128, out_ch=64):
        super(resinception, self).__init__()
        self.convconcat_1=nn.Conv2d(128,out_ch,1,padding=0)
        self.convconcat_3= nn.Conv2d(64, out_ch, 3, padding=1)

        self.conv_r1 = nn.Conv2d(64, out_ch, 3, padding=1, dilation=1)
        self.conv_r2 = nn.Conv2d(64, out_ch, 3, padding=2, dilation=2)
        # self.maxpool3 = nn.MaxPool2d(3, stride=8, ceil_mode=True)
        # self.conv_r5 = nn.Conv2d(in_ch, out_ch, 3, padding=5, dilation=5)
        # self.bn_r1 = nn.BatchNorm2d(out_ch)
        # self.relu_r1 = nn.ReLU(inplace=True)
        # self.relu_s1 = nn.ReLU(inplace=True)
        # self.relu_s1 = nn.ReLU(inplace=True)
        self.convIR = nn.Conv2d(192, 64, kernel_size=1, bias=False)

    def forward(self, x):
        hx = x
        out3=self.convconcat_1(hx)
        out1 = self.conv_r1(self.convconcat_1(hx))
        out2 = self.conv_r2(self.convconcat_1(hx))
        # out3 = self.convconcat_3(self.bn_r1(self.conv_r5(hx)))
        xout=torch.cat((out1,out2,out3), 1)
        xout= self.convIR(xout)
        return xout

class PPM1(nn.Module):
    def __init__(self, in_dim, out_dim, bins):
        super(PPM1, self).__init__()
        self.features = []
        for bin in bins:
            self.features.append(nn.Sequential(
                nn.AdaptiveAvgPool2d(bin),
                nn.Conv2d(in_dim, out_dim, kernel_size=1, padding=0,bias=False),
                nn.BatchNorm2d(out_dim),
                nn.ReLU(inplace=True)
            ))
        self.features = nn.ModuleList(self.features)
        self.maxpool=nn.MaxPool2d(2, stride=4, ceil_mode=True)
        self.convc=nn.Conv2d(256,64,kernel_size=1, bias=False)

    def forward(self, x):
        x_size = x.size()
        out = [x]
        for f in self.features:
            temp = f(x)
            temp = F.interpolate(temp, x_size[2:], mode="bilinear", align_corners=True)
            out.append(temp)
            map1=torch.cat(out, 1)
        map1=self.maxpool(map1)
        map1=self.convc(map1)

        # return torch.cat(out, 1)#不同尺寸图像放大后相连
        return map1
class PPM2(nn.Module):
    def __init__(self, in_dim, out_dim, bins):
        super(PPM2, self).__init__()
        self.features = []
        for bin in bins:
            self.features.append(nn.Sequential(
                nn.AdaptiveAvgPool2d(bin),
                nn.Conv2d(in_dim, out_dim, kernel_size=1, bias=False),
                nn.BatchNorm2d(out_dim),
                nn.ReLU(inplace=True)
            ))
        self.features = nn.ModuleList(self.features)
        self.maxpool=nn.MaxPool2d(2, stride=8, ceil_mode=True)
        self.convc=nn.Conv2d(256,64,kernel_size=1,padding=0, bias=False)

    def forward(self, x):
        x_size = x.size()
        out = [x]
        for f in self.features:
            temp = f(x)
            temp = F.interpolate(temp, x_size[2:], mode="bilinear", align_corners=True)
            out.append(temp)
            map2=torch.cat(out, 1)
        map2=self.maxpool(map2)
        map2=self.convc(map2)

        # return torch.cat(out, 1)#不同尺寸图像放大后相连
        return map2
class REBNCONV(nn.Module):
    def __init__(self, in_ch=3, out_ch=3, dirate=1):
        super(REBNCONV, self).__init__()

        self.conv_s1 = nn.Conv2d(in_ch, out_ch, 3, padding=1 * dirate, dilation=1 * dirate)

        self.bn_s1 = nn.BatchNorm2d(out_ch)

        # self.relu_s1 = nn.ReLU(inplace=True)
        self.relu_s1 = nn.ReLU(inplace=True)

    def forward(self, x):
        hx = x
        xout = self.relu_s1(self.bn_s1(self.conv_s1(hx)))

        return xout
lass preconv(nn.Module):

    def __init__(self, in_ch=3, out_ch=2):
        super(preconv, self).__init__()

        self.pgru=ConvGRU(3,64)
        self.stage1 = RSU7(in_ch, 16, 64)
        self.outconv = nn.Conv2d(64, out_ch, 1)
    def forward(self, x):
        hx = x
        hx1 = self.stage1(hx)
        tx=x
        d0=self.pgru(tx,hx1)
        d0=self.outconv(d0)
        return d0
def _upsample_wy(src, tar):
    # src = F.upsample(src,size=tar.shape[2:],mode='bilinear')  # bilinear               paper 双线性插值的方式进行上采样
    src = F.upsample(src, size=tar.shape[2:], mode='bilinear')  #              mine

    return src

### RSU-7 ###
class RSU7(nn.Module):  # UNet07DRES(nn.Module):

    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU7, self).__init__()

        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)  # Conv2d--->BatchNorm2d--->ReLU

        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 最大池化（max-pooling）即取局部接受域中值最大的点

        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=1)

        self.rebnconv7 = REBNCONV(mid_ch, mid_ch, dirate=2)

        self.rebnconv6d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv5d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv4d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv3d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)

        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)

        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)

        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)

        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)

        hx5 = self.rebnconv5(hx)
        hx = self.pool5(hx5)

        hx6 = self.rebnconv6(hx)

        hx7 = self.rebnconv7(hx6)

        hx6d = self.rebnconv6d(torch.cat((hx7, hx6), 1))
        hx6dup = _upsample_like(hx6d, hx5)

        hx5d = self.rebnconv5d(torch.cat((hx6dup, hx5), 1))
        hx5dup = _upsample_like(hx5d, hx4)

        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = _upsample_like(hx4d, hx3)

        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)

        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)

        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))

        return hx1d + hxin


### RSU-6 ###
class RSU6(nn.Module):  # UNet06DRES(nn.Module):

    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU6, self).__init__()

        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)

        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)

        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=2)

        self.rebnconv5d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv4d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv3d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x):
        hx = x

        hxin = self.rebnconvin(hx)

        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)

        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)

        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)

        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)

        hx5 = self.rebnconv5(hx)

        hx6 = self.rebnconv6(hx5)

        hx5d = self.rebnconv5d(torch.cat((hx6, hx5), 1))
        hx5dup = _upsample_like(hx5d, hx4)

        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = _upsample_like(hx4d, hx3)

        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)

        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)

        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))

        return hx1d + hxin


### RSU-5 ###
class RSU5(nn.Module):  # UNet05DRES(nn.Module):

    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU5, self).__init__()

        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)

        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)

        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=2)

        self.rebnconv4d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv3d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x):
        hx = x

        hxin = self.rebnconvin(hx)

        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)

        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)

        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)

        hx4 = self.rebnconv4(hx)

        hx5 = self.rebnconv5(hx4)

        hx4d = self.rebnconv4d(torch.cat((hx5, hx4), 1))
        hx4dup = _upsample_like(hx4d, hx3)

        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)

        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)

        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))

        return hx1d + hxin


### RSU-4 ###
class RSU4(nn.Module):  # UNet04DRES(nn.Module):

    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU4, self).__init__()

        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)

        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)

        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=2)

        self.rebnconv3d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x):
        hx = x

        hxin = self.rebnconvin(hx)

        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)

        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)

        hx3 = self.rebnconv3(hx)

        hx4 = self.rebnconv4(hx3)

        hx3d = self.rebnconv3d(torch.cat((hx4, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)

        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)

        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))

        return hx1d + hxin



class U2NETP_wy(nn.Module):

    def __init__(self, in_ch=3, out_ch=3):
        super(U2NETP_wy, self).__init__()

        self.stage1 = RSU7(in_ch, 16, 64)
        self.unetadd = Unet(num_classes=3)
        # self.unetadd=U_Net_v7_P(3,5)

        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage2 = RSU6(64, 16, 64)

        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage3 = RSU5(64, 16, 64)

        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage4 = RSU4(64, 16, 64)

        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage5 = RSU4F(64, 16, 64)

        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.stage6 = RSU4F(64, 16, 64)

        # decoder
        self.stage5d = RSU4F(192, 16, 64)#修改了输入通道数
        self.stage4d = RSU4(192, 16, 64)
        self.stage3d = RSU5(192, 16, 64)
        self.stage2d = RSU6(192, 16, 64)
        self.stage1d = RSU7(192, 16, 64)
        # self.stage5d = RSU4F(128, 16, 64)  # 修改了输入通道数
        # self.stage4d = RSU4(128, 16, 64)
        # self.stage3d = RSU5(128, 16, 64)
        # self.stage2d = RSU6(128, 16, 64)
        # self.stage1d = RSU7(128, 16, 64)


        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side3 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side4 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side5 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side6 = nn.Conv2d(64, out_ch, 3, padding=1)

        # self.outconv = nn.Conv2d(35, out_ch, 1)#修改了输入数
        # self.outconv = nn.Conv2d(35, out_ch, 1)  # 修改了输入数
        self.outconv = nn.Conv2d(21,3, 1)
        # self.outconv = nn.Conv2d(30, out_ch, 1)
        self.pspnet1 = PPM1(64, 64, [32, 64, 128])
        self.pspnet2 = PPM2(64, 64, [32, 64, 128])

        self.concat_encoder1 = nn.Conv2d(128, 64, 1, stride=1, padding=0)
        self.concat_encoder2 = nn.Conv2d(192, 64, 1, stride=1, padding=0)
        self.concat_decoder = nn.Conv2d(128, 64, 1, stride=1, padding=0)
        self.resin=resinception(128,64)
        self.wygru=ConvGRU(128,64)
        self.outadd = nn.Conv2d(10, out_ch, 3, padding=1)


    def forward(self, x):

        ux=x
        d00=self.unetadd(ux)
        hx = x

        # stage 1
        hx1 = self.stage1(hx)

        hx13 = self.pspnet1(hx1)  # 要进入三

        hx14 = self.pspnet2(hx1)


        hx = self.pool12(hx1)

        # stage 2
        hx2 = self.stage2(hx)

        hx24 = self.pspnet1(hx2)

        hx25 = self.pspnet2(hx2)

        hx = self.pool23(hx2)


        # stage 3

        hx = torch.cat((hx, hx13), 1)
        hx = self.concat_encoder1(hx)
        hx3 = self.stage3(hx)

        hx35 = self.pspnet1(hx3)

        hx36 = self.pspnet2(hx3)

        hx = self.pool34(hx3)


        # stage 4

        hx = torch.cat((hx,hx14, hx24), 1)
        hx = self.concat_encoder2(hx)
        hx4 = self.stage4(hx)
        hx46 = self.pspnet1(hx4)

        hx = self.pool45(hx4)


        # stage 5
        hx = torch.cat((hx, hx25, hx35), 1)
        hx = self.concat_encoder2(hx)
        hx5 = self.stage5(hx)

        hx = self.pool56(hx5)


        # stage 6
        hx = torch.cat((hx, hx36, hx46), 1)
        hx = self.concat_encoder2(hx)
        hx6 = self.stage6(hx)
        hx6up = _upsample_like(hx6, hx5)



        ############################## decoder
        hx65 = _upsample_wy(hx6, hx5)
        hx65_add= torch.add(hx65, hx5)
        hx65_multi=torch.mul(hx65,hx5)
        hx65_IR=torch.cat((hx65_add,hx65_multi),1)

        hx5in=self.wygru(hx65_IR,hx5)
        hx5d = self.stage5d(torch.cat((hx6up, hx5,hx5in), 1))
        hx5dup = _upsample_like(hx5d, hx4)


        hx54 = _upsample_wy(hx5, hx4)
        hx54_add = torch.add(hx54, hx4)
        hx54_multi = torch.mul(hx54, hx4)
        hx54_IR = torch.cat((hx54_add, hx54_multi), 1)

        hx4in = self.wygru(hx54_IR, hx4)
        hx4d = self.stage4d(torch.cat((hx5dup, hx4,hx4in), 1))
        hx4dup = _upsample_like(hx4d, hx3)


        hx43 = _upsample_wy(hx4, hx3)
        hx43_add = torch.add(hx43, hx3)
        hx43_multi = torch.mul(hx43, hx3)
        hx43_IR = torch.cat((hx43_add, hx43_multi), 1)

        hx3in = self.wygru(hx43_IR, hx3)
        hx3d = self.stage3d(torch.cat((hx4dup, hx3,hx3in), 1))
        hx3dup = _upsample_like(hx3d, hx2)


        hx32 = _upsample_wy(hx3, hx2)
        hx32_add = torch.add(hx32, hx2)
        hx32_multi = torch.mul(hx32, hx2)
        hx32_IR = torch.cat((hx32_add, hx32_multi), 1)

        hx2in = self.wygru(hx32_IR, hx2)
        hx2d = self.stage2d(torch.cat((hx3dup, hx2,hx2in), 1))
        hx2dup = _upsample_like(hx2d, hx1)


        hx21 = _upsample_wy(hx2, hx1)
        hx21_add = torch.add(hx21, hx1)
        hx21_multi = torch.mul(hx21, hx1)
        hx21_IR = torch.cat((hx21_add, hx21_multi), 1)

        hx1in = self.wygru(hx21_IR, hx1)
        hx1d = self.stage1d(torch.cat((hx2dup, hx1,hx1in), 1))
       


        # # side output
        d1 = self.side1(hx1d)
        #
        d2 = self.side2(hx2d)
        # d2 = self.carafe2(d2)
        d2 = _upsample_like(d2, d1)
        #
        d3 = self.side3(hx3d)
        # d3 = self.carafe4(d3)
        d3 = _upsample_like(d3, d1)
        #
        d4 = self.side4(hx4d)
        # d4 = self.carafe8(d4)

        d4 = _upsample_like(d4, d1)
        #
        d5 = self.side5(hx5d)
        # d5 = self.carafe16(d5)
        d5 = _upsample_like(d5, d1)
        #
        d6 = self.side6(hx6)
        # d6=self.carafe32(d6)

        d6 = _upsample_like(d6, d1)
        #

        d0 = self.outconv(torch.cat((d1, d2, d3, d4, d5, d6,d00), 1))#5*H*W直接加模块即可
        

        return d0
